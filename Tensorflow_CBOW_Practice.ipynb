{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import string\n",
    "import io\n",
    "import collections\n",
    "import urllib2\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess=tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=50\n",
    "embedding_size=200\n",
    "vocabulary_size=10000\n",
    "generations=50000\n",
    "model_learning_rate = 0.001 # Learning rate\n",
    "print_loss_every=500\n",
    "num_sampled=int(batch_size/2)\n",
    "window_size=2\n",
    "stops=stopwords.words('english')\n",
    "print_valid_every=2000\n",
    "valid_words=['cliche','love','hate','silly','sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos=open('temp/rt-polaritydata/rt-polarity.pos','r')\n",
    "neg=open('temp/rt-polaritydata/rt-polarity.neg','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_data=[]\n",
    "neg_data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in pos:\n",
    "    pos_data.append(line.decode('ISO-8859-1').encode('utf-8',errors='ignore').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in neg:\n",
    "    neg_data.append(line.decode('ISO-8859-1').encode('utf-8',errors='ignore').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts=pos_data+neg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target=[1]*len(pos_data)+[0]*len(neg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(texts,stops):\n",
    "    # This is the function that we use to normalize the texts\n",
    "    # Lower all the cases\n",
    "    texts=[x.lower() for x in texts]\n",
    "    # Remove all the punctuation\n",
    "    texts=[''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "    # Remove all the stops words\n",
    "    texts=[' '.join([word for word in x.split() if word not in stops]) for x in texts]\n",
    "    # Remove numbers\n",
    "    texts=[''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "    # Remove extra white space\n",
    "    texts=[' '.join(x.split()) for x in texts]\n",
    "    return texts\n",
    "texts=normalize_text(texts,stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To make sure all the movies are informative, here we only kept the longer sentences\n",
    "target = [target[ix] for ix,x in enumerate(texts) if len(x.split())>2]\n",
    "texts = [x for x in texts if len(x.split())>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_sentences = [s.split() for s in texts]\n",
    "words = [x for sublist in split_sentences for x in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count=[['RARE',-1]]\n",
    "count.extend(collections.Counter(words).most_common(vocabulary_size-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict={}\n",
    "for word,word_count in count:\n",
    "    word_dict[word]=len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dictionary(sentences,vocabulary_size):\n",
    "    # create sentences into list of words\n",
    "    split_sentences = [s.split() for s in sentences]\n",
    "    words = [x for sublist in split_sentences for x in sublist]\n",
    "    count=[['RARE',-1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
    "    word_dict={}\n",
    "    for word,word_count in count:\n",
    "        word_dict=len(word_dict)\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we create sentence into numbers so we can use it for later training\n",
    "def text_to_numbers(sentences,word_dict):\n",
    "    data=[]\n",
    "    for sentence in sentences:\n",
    "        temp=[]\n",
    "        for word in sentence:\n",
    "            if word in word_dict:\n",
    "                word_ix=word_dict[word]\n",
    "            else:\n",
    "                word_ix=0\n",
    "            temp.append(word_ix)\n",
    "        data.append(temp)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_dictionary=word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dictionary_rev=dict(zip(word_dictionary.values(),word_dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data=text_to_numbers(texts,word_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_examples=[word_dictionary[x] for x in valid_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rand_sentence=np.random.choice(text_data)\n",
    "#generate consecutive window to look at\n",
    "window_sequences=[rand_sentence[max(ix-window_size,0):(ix+window_size+1)] for ix,x in enumerate(rand_sentence)]\n",
    "label_indices=[ix if ix<window_size else window_size for ix,x in enumerate(window_sequences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_and_labels=[(x[y],x[:y]+x[(y+1):]) for x,y in zip(window_sequences,label_indices)]\n",
    "tuple_data=[(x,y_) for x,y in batch_and_labels for y_ in y]\n",
    "batch,labels = [list(x) for x in zip(*tuple_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_and_labels=[(x[:y]+x[(y+1):],x[y]) for x,y in zip(window_sequences,label_indices)]\n",
    "tuple_data=[(x,y) for x,y in batch_and_labels if len(x)==2*window_size]\n",
    "batch,labels = [list(x) for x in zip(*tuple_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we need to create skip-gram batches\n",
    "def generate_batch_data(sentences, batch_size,window_size,method='skip-gram'):\n",
    "    # Fill out the batch\n",
    "    batch_data=[]\n",
    "    label_data=[]\n",
    "    while len(batch_data)<batch_size:\n",
    "        rand_sentence=np.random.choice(sentences)\n",
    "        #generate consecutive window to look at\n",
    "        window_sequences=[rand_sentence[max(ix-window_size,0):(ix+window_size+1)] for ix,x in enumerate(rand_sentence)]\n",
    "        label_indices=[ix if ix<window_size else window_size for ix,x in enumerate(window_sequences)]\n",
    "        if method=='skip-gram':\n",
    "            batch_and_labels=[(x[y],x[:y]+x[(y+1):]) for x,y in zip(window_sequences,label_indices)]\n",
    "            tuple_data=[(x,y_) for x,y in batch_and_labels for y_ in y]\n",
    "            batch,labels = [list(x) for x in zip(*tuple_data)]\n",
    "        elif method=='cbow':\n",
    "            batch_and_labels=[(x[:y]+x[(y+1):],x[y]) for x,y in zip(window_sequences,label_indices)]\n",
    "            tuple_data=[(x,y) for x,y in batch_and_labels if len(x)==2*window_size]\n",
    "            batch,labels = [list(x) for x in zip(*tuple_data)]\n",
    "        #extract batch and labels\n",
    "        batch_data.extend(batch[:batch_size])\n",
    "        label_data.extend(labels[:batch_size])\n",
    "    batch_data=batch_data[:batch_size]\n",
    "    label_data=label_data[:batch_size]\n",
    "    \n",
    "    batch_data=np.array(batch_data)\n",
    "    label_data=np.array(np.array([label_data]))\n",
    "    \n",
    "    return (batch_data,label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n"
     ]
    }
   ],
   "source": [
    "print('Creating Model')\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))\n",
    "\n",
    "# Nce loss parameters\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size,embedding_size],stddev=1.0/np.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "# Create data/target placeholders\n",
    "x_inputs = tf.placeholder(tf.int32,shape=[batch_size,2*window_size])\n",
    "y_target = tf.placeholder(tf.int32,shape=[batch_size,1])\n",
    "valid_dataset = tf.constant(valid_examples,dtype=tf.int32)\n",
    "\n",
    "# Lookup the word embedding\n",
    "# Add together window embeddings:\n",
    "embed=tf.zeros([batch_size,embedding_size])\n",
    "for element in range(2*window_size):\n",
    "    embed+=tf.nn.embedding_lookup(embeddings,x_inputs[:,element])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get loss from prediction\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                     biases=nce_biases,\n",
    "                                     labels=y_target,\n",
    "                                     inputs=embed,\n",
    "                                     num_sampled=num_sampled,\n",
    "                                     num_classes=vocabulary_size))\n",
    "# Create optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=model_learning_rate).minimize(loss)\n",
    "\n",
    "# Cosine similarity between words\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model saving operation\n",
    "saver = tf.train.Saver({\"embeddings\": embeddings})\n",
    "\n",
    "#Add variable initializer.\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter out sentences that aren't long enough:\n",
    "text_data = [x for x in text_data if len(x)>=(2*window_size+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Loss at step 500 : 77.1532287598\n",
      "Loss at step 1000 : 53.1226654053\n",
      "Loss at step 1500 : 78.4160766602\n",
      "Loss at step 2000 : 74.4537963867\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: olivier, clubs, mockumentary, careful, b,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 2500 : 45.5425567627\n",
      "Loss at step 3000 : 34.4220275879\n",
      "Loss at step 3500 : 36.7954788208\n",
      "Loss at step 4000 : 34.6438179016\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: olivier, clubs, mockumentary, careful, b,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 4500 : 30.678062439\n",
      "Loss at step 5000 : 20.4906959534\n",
      "Loss at step 5500 : 21.7277812958\n",
      "Loss at step 6000 : 29.6749687195\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: olivier, clubs, mockumentary, b, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 6500 : 15.7354507446\n",
      "Loss at step 7000 : 20.4458770752\n",
      "Loss at step 7500 : 12.9583568573\n",
      "Loss at step 8000 : 25.4583244324\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: olivier, clubs, mockumentary, b, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 8500 : 17.0363121033\n",
      "Loss at step 9000 : 23.8466033936\n",
      "Loss at step 9500 : 10.7967214584\n",
      "Loss at step 10000 : 5.03619146347\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: olivier, clubs, mockumentary, b, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 10500 : 22.5185394287\n",
      "Loss at step 11000 : 16.9050559998\n",
      "Loss at step 11500 : 8.47124195099\n",
      "Loss at step 12000 : 8.41883277893\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: olivier, clubs, mockumentary, b, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 12500 : 4.47679710388\n",
      "Loss at step 13000 : 9.68424129486\n",
      "Loss at step 13500 : 10.5167093277\n",
      "Loss at step 14000 : 7.64524650574\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: olivier, clubs, mockumentary, b, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 14500 : 14.365776062\n",
      "Loss at step 15000 : 9.11362075806\n",
      "Loss at step 15500 : 7.25494003296\n",
      "Loss at step 16000 : 4.91007852554\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: olivier, clubs, mockumentary, b, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 16500 : 5.53801345825\n",
      "Loss at step 17000 : 3.71858692169\n",
      "Loss at step 17500 : 9.37039089203\n",
      "Loss at step 18000 : 7.85295057297\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: olivier, clubs, mockumentary, b, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 18500 : 7.90111875534\n",
      "Loss at step 19000 : 2.95204496384\n",
      "Loss at step 19500 : 2.58641529083\n",
      "Loss at step 20000 : 8.01864910126\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: olivier, clubs, b, mockumentary, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 20500 : 4.1288819313\n",
      "Loss at step 21000 : 3.05585169792\n",
      "Loss at step 21500 : 3.98587989807\n",
      "Loss at step 22000 : 4.03593397141\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: olivier, clubs, b, mockumentary, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 22500 : 5.73652267456\n",
      "Loss at step 23000 : 3.4385535717\n",
      "Loss at step 23500 : 2.36483287811\n",
      "Loss at step 24000 : 3.70701885223\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: olivier, clubs, b, mockumentary, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 24500 : 7.186419487\n",
      "Loss at step 25000 : 1.13845968246\n",
      "Loss at step 25500 : 2.77135467529\n",
      "Loss at step 26000 : 3.85021829605\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: olivier, clubs, b, mockumentary, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 26500 : 6.49185323715\n",
      "Loss at step 27000 : 2.24171352386\n",
      "Loss at step 27500 : 2.13002109528\n",
      "Loss at step 28000 : 0.988833487034\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: olivier, clubs, b, mockumentary, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 28500 : 2.76306605339\n",
      "Loss at step 29000 : 1.30234360695\n",
      "Loss at step 29500 : 7.56030464172\n",
      "Loss at step 30000 : 4.43809080124\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: olivier, clubs, b, mockumentary, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 30500 : 2.01027822495\n",
      "Loss at step 31000 : 2.66091370583\n",
      "Loss at step 31500 : 1.15966939926\n",
      "Loss at step 32000 : 2.0585000515\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: olivier, clubs, b, mockumentary, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 32500 : 3.5971570015\n",
      "Loss at step 33000 : 8.35609722137\n",
      "Loss at step 33500 : 2.10212182999\n",
      "Loss at step 34000 : 3.48468708992\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: olivier, b, clubs, mockumentary, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 34500 : 1.31312131882\n",
      "Loss at step 35000 : 1.34013271332\n",
      "Loss at step 35500 : 4.62646532059\n",
      "Loss at step 36000 : 2.96314930916\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: olivier, b, clubs, mockumentary, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 36500 : 1.16820788383\n",
      "Loss at step 37000 : 3.47824549675\n",
      "Loss at step 37500 : 0.691286087036\n",
      "Loss at step 38000 : 0.959910869598\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: olivier, b, clubs, mockumentary, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 38500 : 1.57126057148\n",
      "Loss at step 39000 : 1.12792515755\n",
      "Loss at step 39500 : 2.02278804779\n",
      "Loss at step 40000 : 0.885532736778\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: olivier, b, clubs, mockumentary, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 40500 : 2.27590632439\n",
      "Loss at step 41000 : 3.18125152588\n",
      "Loss at step 41500 : 1.67457211018\n",
      "Loss at step 42000 : 2.24508929253\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: olivier, b, clubs, mockumentary, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 42500 : 1.23064935207\n",
      "Loss at step 43000 : 1.74017119408\n",
      "Loss at step 43500 : 1.19881808758\n",
      "Loss at step 44000 : 1.61814618111\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: olivier, b, clubs, mockumentary, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 44500 : 1.95188105106\n",
      "Loss at step 45000 : 1.75101161003\n",
      "Loss at step 45500 : 1.1662248373\n",
      "Loss at step 46000 : 1.37676310539\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: b, olivier, clubs, mockumentary, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 46500 : 1.19749224186\n",
      "Loss at step 47000 : 1.45844888687\n",
      "Loss at step 47500 : 1.00227475166\n",
      "Loss at step 48000 : 1.56993722916\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: b, olivier, clubs, mockumentary, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n",
      "Loss at step 48500 : 1.72720754147\n",
      "Loss at step 49000 : 2.05157017708\n",
      "Loss at step 49500 : 1.54731249809\n",
      "Loss at step 50000 : 1.16096532345\n",
      "Nearest to cliche: tonal, asia, terrible, instantly, reasons,\n",
      "Nearest to love: tickets, sara, respite, changed, lick,\n",
      "Nearest to hate: b, olivier, clubs, mockumentary, careful,\n",
      "Nearest to silly: durable, scifi, tackling, consummate, monty,\n",
      "Nearest to sad: confessions, discerned, massive, zhao, previously,\n"
     ]
    }
   ],
   "source": [
    "# Run the CBOW model.\n",
    "print('Starting Training')\n",
    "loss_vec = []\n",
    "loss_x_vec = []\n",
    "for i in range(generations):\n",
    "    batch_inputs, batch_labels = generate_batch_data(text_data, batch_size,\n",
    "                                                                  window_size, method='cbow')\n",
    "    feed_dict = {x_inputs : batch_inputs, y_target : np.transpose(batch_labels)}\n",
    "\n",
    "    # Run the train step\n",
    "    sess.run(optimizer, feed_dict=feed_dict)\n",
    "\n",
    "    # Return the loss\n",
    "    if (i+1) % print_loss_every == 0:\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        loss_vec.append(loss_val)\n",
    "        loss_x_vec.append(i+1)\n",
    "        print('Loss at step {} : {}'.format(i+1, loss_val))\n",
    "      \n",
    "    # Validation: Print some random words and top 5 related words\n",
    "    if (i+1) % print_valid_every == 0:\n",
    "        sim = sess.run(similarity, feed_dict=feed_dict)\n",
    "        for j in range(len(valid_words)):\n",
    "            valid_word = word_dictionary_rev[valid_examples[j]]\n",
    "            top_k = 5 # number of nearest neighbors\n",
    "            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "            log_str = \"Nearest to {}:\".format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = word_dictionary_rev[nearest[k]]\n",
    "                log_str = '{} {},' .format(log_str, close_word)\n",
    "            print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ef79390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss over time\n",
    "plt.plot(loss_x_vec, loss_vec, 'k-')\n",
    "plt.title('Training Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
