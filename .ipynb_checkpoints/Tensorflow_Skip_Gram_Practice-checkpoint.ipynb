{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import string\n",
    "import io\n",
    "import collections\n",
    "import urllib2\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess=tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=50\n",
    "embedding_size=200\n",
    "vocabulary_size=10000\n",
    "generations=50000\n",
    "print_loss_every=500\n",
    "num_sampled=int(batch_size/2)\n",
    "window_size=2\n",
    "stops=stopwords.words('english')\n",
    "print_valid_every=2000\n",
    "valid_words=['cliche','love','hate','silly','sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos=open('temp/rt-polaritydata/rt-polarity.pos','r')\n",
    "neg=open('temp/rt-polaritydata/rt-polarity.neg','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_data=[]\n",
    "neg_data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for line in pos:\n",
    "    pos_data.append(line.decode('ISO-8859-1').encode('utf-8',errors='ignore').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for line in neg:\n",
    "    neg_data.append(line.decode('ISO-8859-1').encode('utf-8',errors='ignore').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts=pos_data+neg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target=[1]*len(pos_data)+[0]*len(neg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(texts,stops):\n",
    "    # This is the function that we use to normalize the texts\n",
    "    # Lower all the cases\n",
    "    texts=[x.lower() for x in texts]\n",
    "    # Remove all the punctuation\n",
    "    texts=[''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "    # Remove all the stops words\n",
    "    texts=[' '.join([word for word in x.split() if word not in stops]) for x in texts]\n",
    "    # Remove numbers\n",
    "    texts=[''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "    # Remove extra white space\n",
    "    texts=[' '.join(x.split()) for x in texts]\n",
    "    return texts\n",
    "texts=normalize_text(texts,stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To make sure all the movies are informative, here we only kept the longer sentences\n",
    "target = [target[ix] for ix,x in enumerate(texts) if len(x.split())>2]\n",
    "texts = [x for x in texts if len(x.split())>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_sentences = [s.split() for s in texts]\n",
    "words = [x for sublist in split_sentences for x in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count=[['RARE',-1]]\n",
    "count.extend(collections.Counter(words).most_common(vocabulary_size-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_dict={}\n",
    "for word,word_count in count:\n",
    "    word_dict[word]=len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dictionary(sentences,vocabulary_size):\n",
    "    # create sentences into list of words\n",
    "    split_sentences = [s.split() for s in sentences]\n",
    "    words = [x for sublist in split_sentences for x in sublist]\n",
    "    count=[['RARE',-1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
    "    word_dict={}\n",
    "    for word,word_count in count:\n",
    "        word_dict=len(word_dict)\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we create sentence into numbers so we can use it for later training\n",
    "def text_to_numbers(sentences,word_dict):\n",
    "    data=[]\n",
    "    for sentence in sentences:\n",
    "        temp=[]\n",
    "        for word in sentence:\n",
    "            if word in word_dict:\n",
    "                word_ix=word_dict[word]\n",
    "            else:\n",
    "                word_ix=0\n",
    "            temp.append(word_ix)\n",
    "        data.append(temp)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dictionary=word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_dictionary_rev=dict(zip(word_dictionary.values(),word_dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data=text_to_numbers(texts,word_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_examples=[word_dictionary[x] for x in valid_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we need to create skip-gram batches\n",
    "def generate_batch_data(sentences, batch_size,window_size,method='skip-gram'):\n",
    "    # Fill out the batch\n",
    "    batch_data=[]\n",
    "    label_data=[]\n",
    "    while len(batch_data)<batch_size:\n",
    "        rand_sentence=np.random.choice(sentences)\n",
    "        #generate consecutive window to look at\n",
    "        window_sequences=[rand_sentence[max(ix-window_size,0):(ix+window_size+1)] for ix,x in enumerate(rand_sentence)]\n",
    "        label_indices=[ix if ix<window_size else window_size for ix,x in enumerate(window_sequences)]\n",
    "        if method=='skip-gram':\n",
    "            batch_and_labels=[(x[y],x[:y]+x[(y+1):]) for x,y in zip(window_sequences,label_indices)]\n",
    "            tuple_data=[(x,y_) for x,y in batch_and_labels for y_ in y]\n",
    "        #extract batch and labels\n",
    "        batch,labels = [list(x) for x in zip(*tuple_data)]\n",
    "        batch_data.extend(batch[:batch_size])\n",
    "        label_data.extend(labels[:batch_size])\n",
    "    batch_data=batch_data[:batch_size]\n",
    "    label_data=label_data[:batch_size]\n",
    "    \n",
    "    batch_data=np.array(batch_data)\n",
    "    label_data=np.array(np.array([label_data]))\n",
    "    \n",
    "    return (batch_data,label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings=tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))\n",
    "# create data/target place holder\n",
    "x_inputs=tf.placeholder(tf.int32,shape=[batch_size])\n",
    "y_target=tf.placeholder(tf.int32,shape=[batch_size,1])\n",
    "valid_dataset=tf.constant(valid_examples,dtype=tf.int32)\n",
    "\n",
    "embed=tf.nn.embedding_lookup(embeddings,x_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nce_weights=tf.Variable(tf.truncated_normal([vocabulary_size,embedding_size],stddev=1.0/np.sqrt(embedding_size)))\n",
    "nce_biases= tf.Variable(tf.zeros([vocabulary_size]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                     biases=nce_biases,\n",
    "                                     labels=y_target,\n",
    "                                     inputs=embed,\n",
    "                                     num_sampled=num_sampled,\n",
    "                                     num_classes=vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm=tf.sqrt(tf.reduce_sum(tf.square(embeddings),1,keep_dims=True))\n",
    "normalized_embeddings=embeddings/norm\n",
    "valid_embeddings=tf.nn.embedding_lookup(normalized_embeddings,valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings,normalized_embeddings,transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 500 : 24.0545425415\n",
      "Loss at step 1000 : 44.2535743713\n",
      "Loss at step 1500 : 2.60917544365\n",
      "Loss at step 2000 : 16.2017765045\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 2500 : 17.9602222443\n",
      "Loss at step 3000 : 15.8866796494\n",
      "Loss at step 3500 : 9.6602268219\n",
      "Loss at step 4000 : 1.132984519\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 4500 : 4.4739985466\n",
      "Loss at step 5000 : 1.59271085262\n",
      "Loss at step 5500 : 2.66259837151\n",
      "Loss at step 6000 : 1.07009840012\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 6500 : 1.67666006088\n",
      "Loss at step 7000 : 1.41143703461\n",
      "Loss at step 7500 : 1.33077788353\n",
      "Loss at step 8000 : 1.34060442448\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 8500 : 4.79795742035\n",
      "Loss at step 9000 : 2.50599360466\n",
      "Loss at step 9500 : 3.23601818085\n",
      "Loss at step 10000 : 0.792820990086\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 10500 : 2.20995879173\n",
      "Loss at step 11000 : 0.494902342558\n",
      "Loss at step 11500 : 1.21111059189\n",
      "Loss at step 12000 : 1.00726628304\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 12500 : 2.1724998951\n",
      "Loss at step 13000 : 1.24818229675\n",
      "Loss at step 13500 : 1.61251449585\n",
      "Loss at step 14000 : 0.34371098876\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 14500 : 0.783747494221\n",
      "Loss at step 15000 : 1.1663711071\n",
      "Loss at step 15500 : 2.24475741386\n",
      "Loss at step 16000 : 6.29917860031\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 16500 : 2.3734023571\n",
      "Loss at step 17000 : 1.1403273344\n",
      "Loss at step 17500 : 2.25585985184\n",
      "Loss at step 18000 : 1.70081067085\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 18500 : 2.41753530502\n",
      "Loss at step 19000 : 1.14039039612\n",
      "Loss at step 19500 : 1.41211271286\n",
      "Loss at step 20000 : 0.724763035774\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 20500 : 1.03616893291\n",
      "Loss at step 21000 : 1.54025793076\n",
      "Loss at step 21500 : 2.29150104523\n",
      "Loss at step 22000 : 1.16409814358\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 22500 : 3.72052121162\n",
      "Loss at step 23000 : 3.58550596237\n",
      "Loss at step 23500 : 1.94673252106\n",
      "Loss at step 24000 : 0.698883056641\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 24500 : 1.64478456974\n",
      "Loss at step 25000 : 1.2442677021\n",
      "Loss at step 25500 : 1.00582921505\n",
      "Loss at step 26000 : 2.35467505455\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 26500 : 2.11927437782\n",
      "Loss at step 27000 : 1.24722242355\n",
      "Loss at step 27500 : 1.10346114635\n",
      "Loss at step 28000 : 1.26011610031\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 28500 : 1.3054318428\n",
      "Loss at step 29000 : 2.37316536903\n",
      "Loss at step 29500 : 1.86138916016\n",
      "Loss at step 30000 : 0.788689374924\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 30500 : 1.22418940067\n",
      "Loss at step 31000 : 0.761420667171\n",
      "Loss at step 31500 : 0.888583898544\n",
      "Loss at step 32000 : 1.91578352451\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 32500 : 2.70062947273\n",
      "Loss at step 33000 : 0.846382975578\n",
      "Loss at step 33500 : 1.84726548195\n",
      "Loss at step 34000 : 1.67523515224\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 34500 : 0.591015100479\n",
      "Loss at step 35000 : 1.56083583832\n",
      "Loss at step 35500 : 1.29215443134\n",
      "Loss at step 36000 : 1.22120082378\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 36500 : 1.08870708942\n",
      "Loss at step 37000 : 1.66957056522\n",
      "Loss at step 37500 : 3.08622050285\n",
      "Loss at step 38000 : 0.905573129654\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 38500 : 1.10732007027\n",
      "Loss at step 39000 : 1.0183583498\n",
      "Loss at step 39500 : 0.933350920677\n",
      "Loss at step 40000 : 1.28433990479\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 40500 : 2.28856348991\n",
      "Loss at step 41000 : 0.928633928299\n",
      "Loss at step 41500 : 1.16710054874\n",
      "Loss at step 42000 : 1.60857987404\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 42500 : 0.628044307232\n",
      "Loss at step 43000 : 0.998957574368\n",
      "Loss at step 43500 : 2.36761713028\n",
      "Loss at step 44000 : 0.643355429173\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 44500 : 1.04424703121\n",
      "Loss at step 45000 : 0.714385390282\n",
      "Loss at step 45500 : 1.10644924641\n",
      "Loss at step 46000 : 1.38531982899\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 46500 : 1.00802075863\n",
      "Loss at step 47000 : 1.9454792738\n",
      "Loss at step 47500 : 0.637205600739\n",
      "Loss at step 48000 : 1.10983002186\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n",
      "Loss at step 48500 : 5.05223989487\n",
      "Loss at step 49000 : 1.229205966\n",
      "Loss at step 49500 : 2.05239748955\n",
      "Loss at step 50000 : 1.90097284317\n",
      "Nearest to cliche: witherspoon, campus, sexist, deeper, outlandish,\n",
      "Nearest to love: independent, kissinger, vulgar, cox, sensationalism,\n",
      "Nearest to hate: finished, reggio, illusion, continues, pubescent,\n",
      "Nearest to silly: british, exhilarating, que, aspirations, parking,\n",
      "Nearest to sad: avary, glow, payne, handful, presented,\n"
     ]
    }
   ],
   "source": [
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
    "init=tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "loss_vec = []\n",
    "loss_x_vec = []\n",
    "for i in range(generations):\n",
    "    batch_inputs, batch_labels = generate_batch_data(text_data, batch_size, window_size)\n",
    "    feed_dict = {x_inputs : batch_inputs, y_target : np.transpose(batch_labels)}\n",
    "\n",
    "    # Run the train step\n",
    "    sess.run(optimizer, feed_dict=feed_dict)\n",
    "\n",
    "    # Return the loss\n",
    "    if (i+1) % print_loss_every == 0:\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        loss_vec.append(loss_val)\n",
    "        loss_x_vec.append(i+1)\n",
    "        print(\"Loss at step {} : {}\".format(i+1, loss_val))\n",
    "      \n",
    "    # Validation: Print some random words and top 5 related words\n",
    "    if (i+1) % print_valid_every == 0:\n",
    "        sim = sess.run(similarity, feed_dict=feed_dict)\n",
    "        for j in range(len(valid_words)):\n",
    "            valid_word = word_dictionary_rev[valid_examples[j]]\n",
    "            top_k = 5 # number of nearest neighbors\n",
    "            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "            log_str = \"Nearest to {}:\".format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = word_dictionary_rev[nearest[k]]\n",
    "                log_str = \"%s %s,\" % (log_str, close_word)\n",
    "            print(log_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
