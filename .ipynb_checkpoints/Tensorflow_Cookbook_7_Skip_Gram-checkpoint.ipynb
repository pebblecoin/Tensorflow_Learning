{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import string\n",
    "import requests\n",
    "import collections\n",
    "import io\n",
    "import tarfile\n",
    "import urllib2\n",
    "from nltk.corpus import stopwords\n",
    "sess=tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=50\n",
    "embedding_size=200\n",
    "vocabulary_size=10000\n",
    "generations=50000\n",
    "print_loss_every=500\n",
    "num_sampled=int(batch_size/2)\n",
    "window_size=2\n",
    "stops=stopwords.words('english')\n",
    "print_valid_every=2000\n",
    "valid_words=['cliche','love','hate','silly','sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-da1435a4bd26>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-da1435a4bd26>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    if no s:\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def load_movie_data():\n",
    "    save_folder_name='temp'\n",
    "    pos_file=os.path.join(save_folder_name,'rt-polarity.pos')\n",
    "    neg_file=os.path.join(save_folder_name,'rf-polarity.neg')\n",
    "    # Check if files are already downloaded\n",
    "    if os.path.exists(save_folder_name):\n",
    "        pos_data=[]\n",
    "        with open(pos_file,'r') as temp_pos_file:\n",
    "            for row in temp_pos_file:\n",
    "                pos_data.append(row)\n",
    "        neg_data=[]\n",
    "        with open(neg_file,'r') as temp_neg_file:\n",
    "            for row in temp_neg_file:\n",
    "                neg_data.append(row)\n",
    "    else:\n",
    "        movie_data_url='http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'\n",
    "        stream_data=urllib2.urlopen(movie_data_url)\n",
    "        tmp=io.BytesIO()\n",
    "        while True:\n",
    "            s = stream_data.read(6384)\n",
    "            if not s:\n",
    "                break\n",
    "            temp.write(s)\n",
    "            stream_data.close()\n",
    "            tmp.seek(0)\n",
    "        tar_file=tarfile.open(fileobj=temp.mode='r:gz')\n",
    "        pos=tar_file.extractfile('rt-polaritydata/rt-polarity.pos')\n",
    "        neg=tar_file.extractfile('rt-polaritydata/rt-polarity.neg')\n",
    "        pos_data=[]\n",
    "        for line in pos:\n",
    "            pos_data.append(line.decode('ISO-8895-1').encode('ascii',errors='ignore').decode())\n",
    "            neg_data=[]\n",
    "        for line in neg:\n",
    "            neg_data.append(line.decode('ISO-8895-1').encode('ascii',errors='ignore').decode())\n",
    "        tar_file.close()\n",
    "        # write to file\n",
    "        if not os.path.exists(save_folder_name):\n",
    "            os.makedirs(save_folder_name)\n",
    "        # save files\n",
    "        with open(pos_file,'w') as pos_file_handler:\n",
    "            pos_file_handler.write(''.join(pos_data))\n",
    "        with open(neg_file,'w') as neg_file_handler:\n",
    "            neg_file_handler.write(''.join(neg_data))\n",
    "    texts=post_data+neg_data\n",
    "    target=[1]*len(pos_data) + [0]*len(neg_data)\n",
    "    return(texts,target)\n",
    "texts,target=load_movie_data()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(texts,stops):\n",
    "    texts=[x.lower() for x in texts()]\n",
    "    tests=[''.join(c for c in x if x not in string.punctuation) for x in texts]\n",
    "    texts=[''.join(c for x in x if c not in '0123456789') for x in texts]\n",
    "    texts=[' '.join([word for wod in x.split() if word not in (stops)]) for x in texts]\n",
    "    texts=[' '.join(x.split()) for x in texts]\n",
    "    return (texts)\n",
    "texts=normalize_text(texts,stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target=[target[ix] for ix, x in enumerate(texts) if len(x.split())>2]\n",
    "texts=[x for x in texts if len(x.split())>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dictionary(sentences,vocabulary_size):\n",
    "    split_sentences=[s.split() for s in sentences]\n",
    "    words = [x for sublist in split_sentences for x in sublist]\n",
    "    count=[['RARE',-1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
    "    word_dict={}\n",
    "    for word, word_count in count:\n",
    "        word_dict[word]=len(word_dict)\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_numbers(sentences,word_dict):\n",
    "    data=[]\n",
    "    for sentence in sentences:\n",
    "        sentence_data=[]\n",
    "        for word in word_dict:\n",
    "            if word in word_dict:\n",
    "                word_ix = word_dict[word]\n",
    "            else:\n",
    "                word_ix = 0\n",
    "            sentence_data.append(word_ix)\n",
    "        data.append(sentence_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2a721e227231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_dictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabularty_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mword_dictionary_rev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_to_numbers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_dictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build_dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "word_dictionary=build_dictionary(texts,vocabulary_size)\n",
    "word_dictionary_rev=dict(zip(word_dictionary.values(),word_dictionary.keys()))\n",
    "text_data=text_to_numbers(texts,word_dictionary)\n",
    "valid_examples = [word_dictionary[x] for x in valid words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings=tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))\n",
    "x_inputs=tf.placeholder(tf.int32,shape=[batch_size,2*window_size])\n",
    "y_target=tf.placeholder(tf.int32,shape=[batch_size,1])\n",
    "valid_dataset=tf.constant(valid_examples,dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed=tf.zeros([batch_size,embedding_size])\n",
    "for element in range(2*window_size):\n",
    "    embed += tf.nn.embedding_lookup(embeddings,x_inputs[:,element])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nce_weight=tf.Variable(tf.truncated_normal([vocabulary_size,embedding_size],stddev=1.0/np.sqrt(embedding_size)))\n",
    "nce_biases=tf.Variable(tf.zeros([vocabulary_size]))\n",
    "loss=tf.reduce_mean(tf.nn.nce_loss(nce_weights,nce_biases,embed,y_target,num_smapled,vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings),1,keep_dims=True))\n",
    "normalized_embeddings=embeddings/norm\n",
    "valid_embeddings=tf.nn.embedding_lookup(normalized_embeddings,valid_dataset)\n",
    "similarity=tf.matmul(valid_embeddings,normalized_embeddings,transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver=tf.train.Saver({'embeddings': embeddings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=model_learning_rate).minimize(loss)\n",
    "init=tf.initialize_all_variables()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_vec=[]\n",
    "loss_x_vec=[]\n",
    "for i in range(generations):\n",
    "    batch_inputs,batch_labels = text_helpers.generate_batch_data(text_data,batch_size,windwos_size)\n",
    "    feed_dict={x_inputs:batch_inputs,y_target:batch_labels}\n",
    "    sess.run(optimizer,feed_dict=feed_dict)\n",
    "    if (i+1)% print_loss_every ==0:\n",
    "        loss_val = sess.run(loss,feed_dict=feed_dict)\n",
    "        loss_vec.append(loss_val)\n",
    "        loss_x_vec.append(i+1)\n",
    "        print(\"Loss at step {}:{}\".format(i+1,loss_val))\n",
    "        \n",
    "    if (i+1)% print_valid_every == 0:\n",
    "        sim=sess.run(similarity,feed_dict=feed_dict)\n",
    "        for j in range(len(valid_words)):\n",
    "            valid_word=word_dictionary_rev[valid_examples[j]]\n",
    "            top_k = 5 \n",
    "            nearest = (-sum[j,:]).argsort()[1:top_k+1]\n",
    "            log_str= \"Nearest to {}:\".format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = word_dictionary_rev[nearest[k]]\n",
    "                log_str = '%s %s' % (log_str,close_word)\n",
    "            print log_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences=['This','movie', 'is', 'really','great']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "window_sequences=[sentences[max(ix-window_size,0):(ix+window_size+1)] for ix,x in enumerate(sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This', 'movie'],\n",
       " ['This', 'movie', 'is'],\n",
       " ['movie', 'is', 'really'],\n",
       " ['is', 'really', 'great'],\n",
       " ['really', 'great']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_indices=[ix if ix<window_size else window_size for ix, x in enumerate(window_sequences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp=[[ix,x] for ix,x in enumerate(window_sequences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, ['This', 'movie']],\n",
       " [1, ['This', 'movie', 'is']],\n",
       " [2, ['movie', 'is', 'really']],\n",
       " [3, ['is', 'really', 'great']],\n",
       " [4, ['really', 'great']]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_and_labels=[(x[y],x[:y]+x[(y+1):]) for x,y in zip(window_sequences,label_indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', ['movie']),\n",
       " ('movie', ['This', 'is']),\n",
       " ('is', ['movie', 'really']),\n",
       " ('really', ['is', 'great']),\n",
       " ('great', ['really'])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'really'),\n",
       " ('movie', 'really'),\n",
       " ('is', 'really'),\n",
       " ('really', 'really'),\n",
       " ('great', 'really')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple_data=[(x,y_) for y_ in y for x,y in batch_and_labels ]\n",
    "tuple_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'movie'),\n",
       " ('movie', 'This'),\n",
       " ('movie', 'is'),\n",
       " ('is', 'movie'),\n",
       " ('is', 'really'),\n",
       " ('really', 'is'),\n",
       " ('really', 'great'),\n",
       " ('great', 'really')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple_data=[(x,y_) for x,y in batch_and_labels for y_ in y ]\n",
    "tuple_data"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
