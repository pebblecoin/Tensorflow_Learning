{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import string\n",
    "import io\n",
    "import collections\n",
    "import urllib2\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess=tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=50\n",
    "embedding_size=200\n",
    "vocabulary_size=10000\n",
    "generations=50000\n",
    "print_loss_every=500\n",
    "num_sampled=int(batch_size/2)\n",
    "window_size=2\n",
    "stops=stopwords.words('english')\n",
    "print_valid_every=2000\n",
    "valid_words=['cliche','love','hate','silly','sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos=open('temp/rt-polaritydata/rt-polarity.pos','r')\n",
    "neg=open('temp/rt-polaritydata/rt-polarity.neg','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_data=[]\n",
    "neg_data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for line in pos:\n",
    "    pos_data.append(line.decode('ISO-8859-1').encode('utf-8',errors='ignore').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for line in neg:\n",
    "    neg_data.append(line.decode('ISO-8859-1').encode('utf-8',errors='ignore').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts=pos_data+neg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target=[1]*len(pos_data)+[0]*len(neg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(texts,stops):\n",
    "    # This is the function that we use to normalize the texts\n",
    "    # Lower all the cases\n",
    "    texts=[x.lower() for x in texts]\n",
    "    # Remove all the punctuation\n",
    "    texts=[''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "    # Remove all the stops words\n",
    "    texts=[' '.join([word for word in x.split() if word not in stops]) for x in texts]\n",
    "    # Remove numbers\n",
    "    texts=[''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "    # Remove extra white space\n",
    "    texts=[' '.join(x.split()) for x in texts]\n",
    "    return texts\n",
    "texts=normalize_text(texts,stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To make sure all the movies are informative, here we only kept the longer sentences\n",
    "target = [target[ix] for ix,x in enumerate(texts) if len(x.split())>2]\n",
    "texts = [x for x in texts if len(x.split())>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_sentences = [s.split() for s in texts]\n",
    "words = [x for sublist in split_sentences for x in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count=[['RARE',-1]]\n",
    "count.extend(collections.Counter(words).most_common(vocabulary_size-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_dict={}\n",
    "for word,word_count in count:\n",
    "    word_dict[word]=len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dictionary(sentences,vocabulary_size):\n",
    "    # create sentences into list of words\n",
    "    split_sentences = [s.split() for s in sentences]\n",
    "    words = [x for sublist in split_sentences for x in sublist]\n",
    "    count=[['RARE',-1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
    "    word_dict={}\n",
    "    for word,word_count in count:\n",
    "        word_dict=len(word_dict)\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we create sentence into numbers so we can use it for later training\n",
    "def text_to_numbers(sentences,word_dict):\n",
    "    data=[]\n",
    "    for sentence in sentences:\n",
    "        temp=[]\n",
    "        for word in sentence:\n",
    "            if word in word_dict:\n",
    "                word_ix=word_dict[word]\n",
    "            else:\n",
    "                word_ix=0\n",
    "            temp.append(word_ix)\n",
    "        data.append(temp)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dictionary=word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_dictionary_rev=dict(zip(word_dictionary.values(),word_dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data=text_to_numbers(texts,word_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_examples=[word_dictionary[x] for x in valid_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we need to create skip-gram batches\n",
    "def generate_batch_data(sentences, batch_size,window_size,method='skip-gram'):\n",
    "    # Fill out the batch\n",
    "    batch_data=[]\n",
    "    label_data=[]\n",
    "    while len(batch_data)<batch_size:\n",
    "        rand_sentence=np.random.choice(sentences)\n",
    "        #generate consecutive window to look at\n",
    "        window_sequences=[rand_sentence[max(ix-window_size,0):(ix+window_size+1)] for ix,x in enumerate(rand_sentence)]\n",
    "        label_indices=[ix if ix<window_size else window_size for ix,x in enumerate(window_sequences)]\n",
    "        if method=='skip-gram':\n",
    "            batch_and_labels=[(x[y],x[:y]+x[(y+1):]) for x,y in zip(window_sequences,label_indices)]\n",
    "            tuple_data=[(x,y_) for x,y in batch_and_labels for y_ in y]\n",
    "        #extract batch and labels\n",
    "        batch,labels = [list(x) for x in zip(*tuple_data)]\n",
    "        batch_data.extend(batch[:batch_size])\n",
    "        label_data.extend(labels[:batch_size])\n",
    "    batch_data=batch_data[:batch_size]\n",
    "    label_data=label_data[:batch_size]\n",
    "    \n",
    "    batch_data=np.array(batch_data)\n",
    "    label_data=np.array(np.array([label_data]))\n",
    "    \n",
    "    return (batch_data,label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings=tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))\n",
    "# create data/target place holder\n",
    "x_inputs=tf.placeholder(tf.int32,shape=[batch_size])\n",
    "y_target=tf.placeholder(tf.int32,shape=[batch_size,1])\n",
    "valid_dataset=tf.constant(valid_examples,dtype=tf.int32)\n",
    "\n",
    "embed=tf.nn.embedding_lookup(embeddings,x_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nce_weights=tf.Variable(tf.truncated_normal([vocabulary_size,embedding_size],stddev=1.0/np.sqrt(embedding_size)))\n",
    "nce_biases= tf.Variable(tf.zeros([vocabulary_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                     biases=nce_biases,\n",
    "                                     labels=y_target,\n",
    "                                     inputs=embed,\n",
    "                                     num_sampled=num_sampled,\n",
    "                                     num_classes=vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm=tf.sqrt(tf.reduce_sum(tf.square(embeddings),1,keep_dims=True))\n",
    "normalized_embeddings=embeddings/norm\n",
    "valid_embeddings=tf.nn.embedding_lookup(normalized_embeddings,valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings,normalized_embeddings,transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 500 : 32.9895439148\n",
      "Loss at step 1000 : 2.98992228508\n",
      "Loss at step 1500 : 15.9521713257\n",
      "Loss at step 2000 : 10.6765232086\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 2500 : 8.72713279724\n",
      "Loss at step 3000 : 9.23748397827\n",
      "Loss at step 3500 : 10.3326950073\n",
      "Loss at step 4000 : 10.055606842\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 4500 : 16.3775501251\n",
      "Loss at step 5000 : 4.11282682419\n",
      "Loss at step 5500 : 0.695251166821\n",
      "Loss at step 6000 : 2.22173595428\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 6500 : 2.69490289688\n",
      "Loss at step 7000 : 1.76018321514\n",
      "Loss at step 7500 : 2.72332572937\n",
      "Loss at step 8000 : 0.9196164608\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 8500 : 1.54434478283\n",
      "Loss at step 9000 : 3.93431735039\n",
      "Loss at step 9500 : 3.6209499836\n",
      "Loss at step 10000 : 1.73668789864\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 10500 : 1.74344706535\n",
      "Loss at step 11000 : 0.984026134014\n",
      "Loss at step 11500 : 1.6637250185\n",
      "Loss at step 12000 : 0.834814369678\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 12500 : 2.7877869606\n",
      "Loss at step 13000 : 2.57569336891\n",
      "Loss at step 13500 : 1.22576785088\n",
      "Loss at step 14000 : 0.469990193844\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 14500 : 1.41330456734\n",
      "Loss at step 15000 : 1.16331005096\n",
      "Loss at step 15500 : 2.83603453636\n",
      "Loss at step 16000 : 1.43102383614\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 16500 : 1.4252628088\n",
      "Loss at step 17000 : 1.12290763855\n",
      "Loss at step 17500 : 1.2274929285\n",
      "Loss at step 18000 : 1.53265500069\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 18500 : 0.656627476215\n",
      "Loss at step 19000 : 0.871020793915\n",
      "Loss at step 19500 : 1.10585796833\n",
      "Loss at step 20000 : 1.09838473797\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 20500 : 0.991234898567\n",
      "Loss at step 21000 : 1.62340605259\n",
      "Loss at step 21500 : 1.12734603882\n",
      "Loss at step 22000 : 0.420048415661\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 22500 : 1.08938086033\n",
      "Loss at step 23000 : 0.943780899048\n",
      "Loss at step 23500 : 1.01804494858\n",
      "Loss at step 24000 : 0.928989470005\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 24500 : 1.5740224123\n",
      "Loss at step 25000 : 0.870570838451\n",
      "Loss at step 25500 : 0.392107695341\n",
      "Loss at step 26000 : 1.9204428196\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 26500 : 0.486321210861\n",
      "Loss at step 27000 : 2.66632175446\n",
      "Loss at step 27500 : 1.82571077347\n",
      "Loss at step 28000 : 2.2334959507\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 28500 : 0.778241097927\n",
      "Loss at step 29000 : 0.849791586399\n",
      "Loss at step 29500 : 1.48791325092\n",
      "Loss at step 30000 : 1.45410227776\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 30500 : 1.74234938622\n",
      "Loss at step 31000 : 1.64963388443\n",
      "Loss at step 31500 : 0.69619011879\n",
      "Loss at step 32000 : 1.14359760284\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 32500 : 1.02992808819\n",
      "Loss at step 33000 : 1.04872477055\n",
      "Loss at step 33500 : 1.00766062737\n",
      "Loss at step 34000 : 1.13888323307\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 34500 : 1.04165530205\n",
      "Loss at step 35000 : 1.88629007339\n",
      "Loss at step 35500 : 0.21100486815\n",
      "Loss at step 36000 : 1.5578353405\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 36500 : 1.86906552315\n",
      "Loss at step 37000 : 1.15125072002\n",
      "Loss at step 37500 : 5.83051156998\n",
      "Loss at step 38000 : 1.26027452946\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 38500 : 0.431860387325\n",
      "Loss at step 39000 : 2.17131137848\n",
      "Loss at step 39500 : 0.883383333683\n",
      "Loss at step 40000 : 2.66454553604\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 40500 : 0.295531988144\n",
      "Loss at step 41000 : 0.855721831322\n",
      "Loss at step 41500 : 0.972021460533\n",
      "Loss at step 42000 : 1.59971857071\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 42500 : 1.1315305233\n",
      "Loss at step 43000 : 1.32203555107\n",
      "Loss at step 43500 : 1.16649627686\n",
      "Loss at step 44000 : 2.41940665245\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 44500 : 0.839624762535\n",
      "Loss at step 45000 : 1.09550404549\n",
      "Loss at step 45500 : 0.934421598911\n",
      "Loss at step 46000 : 1.57091939449\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 46500 : 0.674220979214\n",
      "Loss at step 47000 : 0.868548870087\n",
      "Loss at step 47500 : 0.422126501799\n",
      "Loss at step 48000 : 1.46380937099\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n",
      "Loss at step 48500 : 3.28873157501\n",
      "Loss at step 49000 : 1.46856594086\n",
      "Loss at step 49500 : 1.33007979393\n",
      "Loss at step 50000 : 1.24303650856\n",
      "Nearest to cliche: nair, piccoli, mistaken, messenger, strike,\n",
      "Nearest to love: canadians, cannot, eat, greengrass, zany,\n",
      "Nearest to hate: particular, funeral, paperthin, waves, unexpectedly,\n",
      "Nearest to silly: flashback, hammy, workmanlike, band, crack,\n",
      "Nearest to sad: grinning, river, tremendously, twenty, watstein,\n"
     ]
    }
   ],
   "source": [
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)\n",
    "init=tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "loss_vec = []\n",
    "loss_x_vec = []\n",
    "for i in range(generations):\n",
    "    batch_inputs, batch_labels = generate_batch_data(text_data, batch_size, window_size)\n",
    "    feed_dict = {x_inputs : batch_inputs, y_target : np.transpose(batch_labels)}\n",
    "\n",
    "    # Run the train step\n",
    "    sess.run(optimizer, feed_dict=feed_dict)\n",
    "\n",
    "    # Return the loss\n",
    "    if (i+1) % print_loss_every == 0:\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        loss_vec.append(loss_val)\n",
    "        loss_x_vec.append(i+1)\n",
    "        print(\"Loss at step {} : {}\".format(i+1, loss_val))\n",
    "      \n",
    "    # Validation: Print some random words and top 5 related words\n",
    "    if (i+1) % print_valid_every == 0:\n",
    "        sim = sess.run(similarity, feed_dict=feed_dict)\n",
    "        for j in range(len(valid_words)):\n",
    "            valid_word = word_dictionary_rev[valid_examples[j]]\n",
    "            top_k = 5 # number of nearest neighbors\n",
    "            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "            log_str = \"Nearest to {}:\".format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = word_dictionary_rev[nearest[k]]\n",
    "                log_str = \"%s %s,\" % (log_str, close_word)\n",
    "            print(log_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
